{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataUtils\n",
    "# 生成二值化的输入\n",
    "def binary_encoder(input_size):\n",
    "    def wrapper(num):\n",
    "        ret = [int(i) for i in '{0:b}'.format(num)]\n",
    "        return [0] * (input_size - len(ret)) + ret\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# 生成大小为4的向量作为输出\n",
    "def decoder(array):\n",
    "    ret = 0\n",
    "    for i in array:\n",
    "        ret = ret * 2 + int(i)\n",
    "    return ret\n",
    "\n",
    "\n",
    "# 划分训练集和数据集\n",
    "def training_test_gen(x, y):\n",
    "    assert len(x) == len(y)\n",
    "    indices = np.random.permutation(range(len(x)))\n",
    "    split_size = int(0.9 * len(indices))\n",
    "    trX = x[indices[:split_size]]\n",
    "    trY = y[indices[:split_size]]\n",
    "    teX = x[indices[split_size:]]\n",
    "    teY = y[indices[split_size:]]\n",
    "    return trX, trY, teX, teY\n",
    "\n",
    "\n",
    "def get_pytorch_data(input_size=10, limit=1000):\n",
    "    x = []\n",
    "    y = []\n",
    "    encoder = binary_encoder(input_size)\n",
    "    for i in range(limit):\n",
    "        x.append(encoder(i))\n",
    "        if i % 15 == 0:\n",
    "            y.append(0)\n",
    "        elif i % 5 == 0:\n",
    "            y.append(1)\n",
    "        elif i % 3 == 0:\n",
    "            y.append(2)\n",
    "        else:\n",
    "            y.append(3)\n",
    "    return training_test_gen(np.array(x), np.array(y))\n",
    "\n",
    "\n",
    "def get_numpy_data(input_size=10, limit=1000):\n",
    "    x = []\n",
    "    y = []\n",
    "    encoder = binary_encoder(input_size)\n",
    "    for i in range(limit):\n",
    "        x.append(encoder(i))\n",
    "        if i % 15 == 0:\n",
    "            y.append([1, 0, 0, 0])\n",
    "        elif i % 5 == 0:\n",
    "            y.append([0, 1, 0, 0])\n",
    "        elif i % 3 == 0:\n",
    "            y.append([0, 0, 1, 0])\n",
    "        else:\n",
    "            y.append([0, 0, 0, 1])\n",
    "    return training_test_gen(np.array(x), np.array(y))\n",
    "\n",
    "\n",
    "def check_fizbuz(i):\n",
    "    if i % 15 == 0:\n",
    "        return 'fizbuz'\n",
    "    elif i % 5 == 0:\n",
    "        return 'buz'\n",
    "    elif i % 3 == 0:\n",
    "        return 'fiz'\n",
    "    else:\n",
    "        return 'number'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "根据已有数据信息，构建网络的思路：\n",
    "- 将输入数字二值化为10位数字，因此第一个输入层需要10个神经元才能接受这10个数字\n",
    "- 输出始终是大小为4的向量，因此需要4个输出神经元\n",
    "- 首先构建一个结点数位100的隐藏层\n",
    "- 使用64个数据点进行数据的批处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义5个超参数\n",
    "epochs = 500\n",
    "batches = 64\n",
    "lr = 0.01\n",
    "input_size = 10\n",
    "output_size = 4\n",
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trX, trY, teX, teY = get_numpy_data(input_size)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None tensor([[1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 0.]], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 准备输入\n",
    "x = torch.from_numpy(trX).to(device=device, dtype=dtype)\n",
    "y = torch.from_numpy(trY).to(device=device, dtype=dtype)\n",
    "print(x.grad, x.grad_fn, x)\n",
    "# None None tensor([[...]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None tensor([[ 2.6937e-01, -1.2346e+00,  8.7802e-01,  2.4474e+00, -3.8610e-01,\n",
      "          1.4135e+00,  1.0142e+00,  2.4813e-02,  9.3307e-01,  1.0022e+00,\n",
      "         -4.3590e-01, -1.3530e-01,  3.4596e-01,  1.6907e-02, -1.2306e+00,\n",
      "          3.4348e-01,  5.0399e-01, -1.1721e+00,  1.3460e+00, -1.6082e-02,\n",
      "          7.8378e-01,  2.2673e-01, -2.8122e-01,  1.2901e+00,  8.8757e-01,\n",
      "         -1.3858e-01, -9.3914e-01,  1.1388e+00, -3.5818e-01,  1.8353e-02,\n",
      "          1.2423e+00,  6.1580e-01,  9.7386e-01, -1.0645e+00, -1.0333e-01,\n",
      "          1.9774e-01, -3.2640e-01,  9.8945e-01,  2.7390e-01, -2.2330e+00,\n",
      "          1.3827e+00,  1.1603e-01, -4.7850e-01,  6.1885e-01,  6.1713e-02,\n",
      "         -5.9669e-01,  2.5547e-01, -4.9556e-01,  1.0010e-01, -2.6631e-01,\n",
      "         -1.3187e+00, -1.3085e+00, -1.4812e+00,  3.9448e-01, -5.4051e-01,\n",
      "          9.6986e-01, -4.5480e-01,  8.1977e-01,  4.8560e-01,  9.0852e-02,\n",
      "         -1.2053e+00,  2.0573e+00,  2.5806e-01, -2.1518e-01, -3.3136e-01,\n",
      "          1.0085e-01,  1.3768e-01, -1.5038e+00,  2.2438e-02,  6.6988e-01,\n",
      "          8.3218e-01, -7.0595e-01,  1.5475e+00, -1.6430e+00, -2.2158e+00,\n",
      "          1.4080e+00,  1.5656e+00, -1.1799e+00,  1.0178e+00,  1.6868e+00,\n",
      "         -1.5419e+00,  2.2061e-01, -4.4570e-01, -5.8519e-01,  1.3127e-01,\n",
      "          1.6203e+00, -3.3646e-01,  1.1336e+00,  1.0177e+00, -2.6039e-02,\n",
      "          1.8808e+00,  1.0887e+00,  2.8106e-01,  6.8428e-01, -7.5869e-02,\n",
      "         -5.8197e-01, -6.7158e-01,  2.7192e-01,  1.5882e-01,  1.0279e+00],\n",
      "        [ 1.8379e-01,  1.4135e+00, -1.5630e+00, -1.9592e-01, -8.6297e-01,\n",
      "         -8.1617e-01, -2.6680e-01,  3.4995e-01,  2.6348e+00,  7.1024e-01,\n",
      "          8.1986e-01, -1.4042e+00, -2.1480e-01, -7.6254e-01,  9.8505e-01,\n",
      "          1.6402e+00,  9.6980e-01,  1.2192e+00,  2.0374e+00,  8.4668e-01,\n",
      "         -2.0683e+00, -3.4092e-01, -9.1238e-01, -3.9107e-01, -7.9403e-01,\n",
      "          8.7588e-01, -4.6914e-02,  2.9915e-01,  6.3036e-01,  5.7279e-01,\n",
      "         -1.7534e-01,  1.3730e+00,  9.6373e-01,  1.2360e+00, -1.2101e+00,\n",
      "          2.2692e-01,  8.0549e-03,  3.0641e-01, -1.0787e+00,  4.1957e-01,\n",
      "         -1.1947e+00,  8.6801e-01, -8.8499e-01, -7.7167e-01, -1.5315e+00,\n",
      "         -1.6697e-01,  1.1526e+00,  1.4476e-01,  4.2454e-01, -1.7514e-01,\n",
      "          7.8914e-01, -1.0844e-01, -1.3732e+00, -1.5938e+00,  1.3298e-02,\n",
      "          1.2990e+00, -2.0884e+00,  2.0772e+00, -1.5876e+00,  2.6867e-01,\n",
      "         -2.5304e-01,  1.6697e-01,  6.5674e-01, -1.6312e-01,  2.3835e-01,\n",
      "         -2.2783e-01,  1.0649e+00,  1.1348e+00,  1.0301e+00, -9.5529e-01,\n",
      "          8.2428e-01, -2.8550e-01, -2.3082e-01,  9.2241e-02,  8.2857e-01,\n",
      "         -3.0023e-01,  8.3475e-01, -2.0431e+00, -1.0243e+00,  1.4108e+00,\n",
      "          5.8575e-01,  2.3149e-01,  8.3732e-01,  1.6531e+00, -1.2713e+00,\n",
      "         -7.9830e-01, -2.7439e-01,  2.2412e-01, -1.1807e+00,  2.7055e-01,\n",
      "         -2.4712e-01,  1.1455e+00, -4.6360e-01,  1.6012e-01,  6.5481e-03,\n",
      "         -6.2494e-01, -1.9561e+00,  7.1702e-02, -7.9689e-01,  4.9064e-01],\n",
      "        [-6.2091e-01,  7.7055e-01,  1.4727e+00,  7.7007e-01, -5.5898e-01,\n",
      "          1.0160e+00, -1.7690e+00, -6.7647e-01,  1.6632e+00,  4.5431e-01,\n",
      "          2.8759e-02, -7.9113e-01,  1.4555e+00, -3.9374e-01, -2.4229e+00,\n",
      "          1.6071e+00, -8.3920e-02,  7.9046e-01, -8.5732e-01, -8.9202e-01,\n",
      "          1.2850e+00,  5.1974e-01,  1.5697e+00, -1.2945e+00,  1.8062e+00,\n",
      "          5.4214e-01, -1.2816e+00, -3.1106e-01, -6.2955e-01, -1.7965e+00,\n",
      "         -7.6641e-02,  7.4321e-01, -5.8565e-01, -2.4524e-01, -1.1983e+00,\n",
      "          7.3331e-01,  1.5222e+00, -1.4525e+00, -2.9916e-03, -3.3228e-01,\n",
      "          1.8599e+00,  8.1866e-01,  8.3365e-01,  5.8022e-01,  5.1275e-01,\n",
      "         -1.4153e+00,  7.7953e-01,  1.0233e+00, -1.1425e+00, -9.4915e-01,\n",
      "          8.8137e-01, -1.3044e+00,  4.4777e-01,  2.5978e+00, -1.8392e-01,\n",
      "         -1.6619e-01, -5.8898e-01, -1.3316e+00,  7.8276e-01, -6.8945e-01,\n",
      "          1.1336e-01,  1.5255e+00, -9.7760e-01, -4.9562e-01,  6.4516e-01,\n",
      "          9.0244e-01,  8.0203e-02,  5.6705e-01,  6.5878e-01, -1.2808e+00,\n",
      "         -2.7797e-01, -5.4215e-01,  8.0981e-01, -2.5105e-01, -5.0851e-01,\n",
      "         -1.3416e+00, -2.7369e-01,  8.4394e-01, -1.8657e-01,  9.1073e-01,\n",
      "         -8.0435e-01,  1.8573e+00, -1.8510e-01, -2.7569e-01, -2.3189e+00,\n",
      "         -7.3430e-01, -8.3431e-01, -2.6916e-01,  1.6400e+00,  6.0119e-01,\n",
      "         -1.7148e-01,  1.0010e+00,  2.0646e-01, -1.3433e-01,  1.5834e+00,\n",
      "          3.1839e-01,  1.2222e+00,  1.3177e+00, -2.2707e-01,  1.2748e+00],\n",
      "        [-1.1412e-01,  3.9581e-01,  7.0249e-01,  1.5408e+00, -5.7353e-02,\n",
      "          1.3215e+00,  2.0528e-01, -6.1310e-01, -2.3389e-01, -1.0651e+00,\n",
      "          7.9424e-01, -8.8677e-01,  9.9938e-01, -7.2782e-01, -6.7582e-01,\n",
      "         -6.0939e-01, -4.0093e-01, -1.4538e+00,  3.9627e-01, -2.6118e-01,\n",
      "         -7.2361e-01,  1.0754e+00,  2.6026e-02,  1.6101e+00, -4.0848e-01,\n",
      "          1.5071e+00, -2.4355e+00, -9.3238e-01,  1.6768e-02, -3.5716e-01,\n",
      "         -6.2709e-01,  7.8925e-01,  4.7085e-01,  1.8699e+00, -3.0882e-01,\n",
      "         -7.1804e-01,  1.1911e+00, -5.3460e-01,  3.4722e-01, -6.0577e-01,\n",
      "         -8.7476e-01,  1.8025e+00, -1.5633e-02,  2.3062e+00, -1.9386e-01,\n",
      "          1.5359e+00, -1.4385e+00,  1.0965e+00,  3.9502e-01, -2.8304e+00,\n",
      "          5.0801e-01, -1.9764e+00,  5.6201e-01, -2.1614e+00,  3.3704e-01,\n",
      "          6.3552e-01, -2.0204e+00, -1.1454e+00,  1.7321e+00,  7.8866e-01,\n",
      "         -1.0838e+00,  4.6204e-01, -8.3204e-01,  5.4073e-01,  3.4426e-01,\n",
      "         -4.6270e-01, -1.0770e+00,  1.1420e+00, -2.5799e-01,  1.6756e+00,\n",
      "          8.1159e-01,  1.1383e+00, -4.8266e-01, -5.7275e-01, -2.5859e-01,\n",
      "         -4.4248e-01,  3.3434e-01, -9.3605e-01,  1.2458e-01,  2.2116e-01,\n",
      "          7.6047e-01, -1.4034e+00, -2.0114e-01,  6.0159e-02,  2.8650e-01,\n",
      "          1.2682e+00, -1.4865e+00,  3.5748e-02, -1.5236e+00,  1.3865e+00,\n",
      "          1.3240e-01, -9.6094e-01,  2.4712e-01,  3.3614e-01,  4.7922e-01,\n",
      "         -8.8171e-01, -1.1242e-01,  3.7960e-01,  1.3921e+00, -2.8346e-01],\n",
      "        [ 1.2573e+00, -1.1984e+00, -8.2170e-01, -1.2643e-01, -4.1995e-02,\n",
      "          2.1747e+00, -5.3094e-01,  2.4414e-01,  2.0102e+00,  5.3459e-01,\n",
      "          2.0176e-01, -2.4166e+00,  1.0171e-01,  2.6074e+00, -1.8781e-01,\n",
      "         -5.5360e-01,  6.7363e-01, -2.2457e-01, -7.2935e-01, -1.4525e-01,\n",
      "         -6.4124e-01,  1.5940e+00, -1.2462e+00, -5.2079e-01,  2.3846e-02,\n",
      "         -8.8558e-03, -4.3481e-02, -4.3807e-01,  2.2959e+00, -9.7436e-01,\n",
      "         -4.1951e-01,  8.2883e-01, -3.6296e-01,  5.6402e-02, -1.1334e+00,\n",
      "         -6.0929e-01,  1.3420e+00, -1.2821e+00, -2.7918e+00,  5.5417e-01,\n",
      "         -1.6776e-02, -2.1007e-01, -1.9787e-01,  2.6626e-02,  3.3656e-01,\n",
      "          1.3207e+00,  4.0429e-01,  3.2229e+00, -3.5282e-01,  1.7725e+00,\n",
      "          6.7044e-01,  1.7026e+00,  1.4106e+00,  9.9987e-01,  7.3970e-01,\n",
      "          1.0230e+00,  8.2542e-01,  6.4353e-01,  2.3264e-01, -5.5057e-01,\n",
      "         -1.3583e+00,  5.0263e-01,  4.2859e-01, -1.6114e+00, -2.2452e+00,\n",
      "          2.6585e-01,  3.6867e-02,  2.5240e-01, -1.4669e+00,  1.2274e+00,\n",
      "          5.9777e-01, -9.2264e-01,  9.5347e-01, -5.4741e-01,  2.4765e-01,\n",
      "         -1.2852e+00,  1.8402e+00,  2.1444e-01,  2.1351e+00,  1.4140e-02,\n",
      "         -9.3545e-01,  6.6314e-01,  6.6660e-01,  2.0900e-01, -1.2095e+00,\n",
      "         -6.7033e-02, -1.3229e-02,  1.2454e+00, -2.4797e-01,  1.0069e+00,\n",
      "          1.0623e+00, -8.3266e-01, -1.6900e+00,  8.0341e-01,  5.1492e-01,\n",
      "          1.2088e+00, -3.9492e-01, -2.7907e+00,  8.4758e-01,  1.7411e+00],\n",
      "        [ 5.8936e-01, -6.0065e-02, -7.2749e-01,  1.1782e+00, -1.4355e+00,\n",
      "         -7.6729e-01,  2.7490e-01, -8.8688e-02,  4.0041e-01, -6.8361e-01,\n",
      "          2.3249e+00,  4.8872e-02, -2.1822e-01, -9.8380e-01,  3.7797e-01,\n",
      "          1.6985e+00, -8.7404e-01, -8.0181e-02, -2.3669e-01,  1.6134e+00,\n",
      "         -6.3789e-01,  1.9150e+00, -1.7935e+00,  4.6213e-01, -2.7287e-01,\n",
      "          1.4228e+00,  8.3666e-02, -1.5812e+00,  3.9297e-01, -6.4898e-01,\n",
      "          7.2375e-01, -4.2086e-01, -3.2107e-01,  4.8595e-01, -4.8014e-01,\n",
      "          1.4436e+00, -5.5013e-01, -1.1760e-01,  1.2526e+00, -1.2480e+00,\n",
      "         -2.2804e+00,  2.8791e-01,  1.8663e+00, -8.1891e-01,  5.3480e-01,\n",
      "         -4.1326e-01,  3.3668e-01, -2.1603e-01, -5.3373e-01, -2.1009e+00,\n",
      "          9.6756e-01, -5.2299e-03, -3.0630e-01, -1.8261e-01, -3.9371e-01,\n",
      "         -3.8835e-01, -4.1263e-01, -1.0209e+00, -8.1378e-01,  1.0664e+00,\n",
      "         -1.1354e+00, -8.9290e-02, -4.4228e-01, -5.8944e-01,  5.9532e-01,\n",
      "          7.8452e-01,  7.4530e-01, -4.3236e-01, -2.2197e-01, -9.4643e-01,\n",
      "          1.1320e-01,  1.1947e+00, -5.8293e-02, -1.1353e+00,  5.4107e-01,\n",
      "         -9.6228e-01, -3.2859e-01,  1.6883e-01,  2.7263e-01,  9.4364e-01,\n",
      "          6.0441e-01,  9.4266e-01,  1.4541e-01,  8.7317e-01,  3.4977e-01,\n",
      "          1.6183e-01, -4.4431e-01, -2.2629e-01, -1.0984e+00,  9.5429e-01,\n",
      "          6.2779e-01, -1.2197e+00, -4.8123e-01,  1.5000e+00,  8.5130e-01,\n",
      "          9.7946e-01,  3.3134e-01, -4.2982e-01, -4.5991e-01,  6.7907e-01],\n",
      "        [ 7.9680e-01, -4.1262e-01, -8.9578e-01,  2.5123e+00, -1.5715e+00,\n",
      "         -6.9000e-01, -1.5346e+00,  3.2881e-01,  1.4738e+00,  1.9188e+00,\n",
      "         -2.0799e+00,  1.7580e+00,  1.6747e-01,  4.8324e-02,  5.9424e-01,\n",
      "          1.4883e-01, -6.8300e-01,  2.0353e+00, -7.9722e-01, -1.5706e+00,\n",
      "         -2.0489e-01,  1.4224e+00, -3.3848e-02,  1.4390e-01, -3.8630e-01,\n",
      "          1.4902e-01, -3.1422e-02,  1.7418e+00,  4.6440e-01, -4.3325e-01,\n",
      "          6.0443e-01,  8.9049e-01,  4.8076e-03, -5.6137e-01,  8.0995e-01,\n",
      "          8.0428e-01, -3.8906e-01, -1.0500e+00,  4.3736e-02, -1.0757e+00,\n",
      "          2.4150e+00, -2.3799e-01, -1.0898e+00, -1.9761e-01,  3.4504e-01,\n",
      "         -2.4077e+00, -1.8970e-01,  1.1850e+00, -2.1444e+00, -1.6217e+00,\n",
      "          9.6731e-02,  4.7561e-01,  6.5340e-02, -1.8530e+00, -6.5013e-01,\n",
      "          1.4435e+00,  4.6479e-01,  2.8037e-01,  9.5217e-01, -9.3663e-01,\n",
      "          7.3197e-01,  1.4334e+00, -1.2514e+00,  9.2861e-01, -7.8680e-01,\n",
      "          7.6443e-01,  1.8321e+00, -3.2418e-01, -1.3944e+00,  6.7231e-01,\n",
      "          1.2637e+00,  6.9796e-02, -9.7164e-01, -1.3538e+00,  1.6384e+00,\n",
      "          1.7032e-01, -7.0469e-01,  1.0145e+00,  4.5190e-01, -9.4225e-01,\n",
      "         -4.0992e+00, -7.7735e-01,  5.7174e-01, -1.2998e+00, -6.8154e-01,\n",
      "          7.3678e-01, -4.2439e-01,  1.1106e-01, -5.2113e-01, -9.5937e-01,\n",
      "         -2.8150e-03,  4.1476e-01, -1.4863e+00,  3.5261e-01, -1.7860e+00,\n",
      "          2.2457e+00,  3.4552e-02, -1.1809e-01,  8.6818e-01,  3.5818e-01],\n",
      "        [-1.9694e-01, -3.5523e-02, -1.1154e+00,  1.5992e+00,  1.0373e+00,\n",
      "          6.4318e-01,  1.3500e+00,  2.2985e-01, -1.9451e-01,  1.1569e+00,\n",
      "         -5.4292e-01, -6.7633e-01,  6.9315e-01, -1.1452e+00, -3.5904e-02,\n",
      "          8.6083e-01, -3.8819e-01, -6.8060e-02, -2.3102e-01,  3.0582e-01,\n",
      "         -1.9422e-01, -3.7235e-01,  1.4607e+00, -1.6637e-01, -4.9896e-01,\n",
      "         -6.7377e-03, -2.7028e-01,  7.3014e-01,  7.5959e-01,  1.0937e-01,\n",
      "          9.7847e-01, -1.0759e+00,  1.0289e+00,  5.9539e-01,  1.4798e+00,\n",
      "         -2.5496e-01, -7.6889e-01,  1.5164e-01,  4.6947e-02,  1.1121e+00,\n",
      "          7.0109e-01,  5.0461e-01, -6.6041e-01,  1.8746e+00,  2.1695e+00,\n",
      "          3.9310e-01, -2.0785e-01, -1.5312e+00, -6.9343e-01,  1.8674e+00,\n",
      "          9.7810e-01, -5.0072e-01, -9.9477e-01,  5.3331e-01,  4.0359e-01,\n",
      "         -2.4298e-01,  1.3886e-01,  9.6635e-01, -9.0431e-01, -2.1077e-01,\n",
      "         -7.7233e-01, -9.9057e-01, -2.2663e+00,  3.1291e-01, -8.0438e-01,\n",
      "          8.2291e-01, -5.7259e-01, -3.1399e-01,  5.9395e-01,  8.3037e-02,\n",
      "          5.0477e-01,  9.9405e-01, -1.7390e+00,  7.9533e-01,  8.1386e-01,\n",
      "          1.5984e+00, -8.6380e-01,  1.1028e-01,  1.4936e+00, -2.5753e-01,\n",
      "         -5.4084e-01,  7.9550e-01,  1.9963e-01, -2.4330e+00,  2.5688e-01,\n",
      "          2.0985e-01,  1.4316e+00,  2.7370e-01, -8.5463e-01, -1.2648e+00,\n",
      "          1.1129e-01,  1.6933e+00, -2.6809e-01,  1.8841e+00,  4.1049e-01,\n",
      "          1.0643e+00,  2.6011e-01,  2.4922e+00, -5.5340e-01,  1.0076e+00],\n",
      "        [ 9.7889e-01,  1.6394e+00,  7.3748e-02,  4.6039e-01, -1.6024e+00,\n",
      "          7.6619e-01, -1.9446e+00,  1.0400e+00, -1.7366e+00,  1.8596e+00,\n",
      "         -3.5369e-01, -1.0929e+00, -5.6776e-01,  1.1322e-01, -2.2878e-01,\n",
      "          6.6666e-01,  1.4224e+00,  5.4981e-01, -4.8621e-02, -1.0510e+00,\n",
      "         -3.4906e-01,  2.1861e+00, -3.8471e-01,  1.7587e+00,  7.0739e-01,\n",
      "         -3.9204e-01, -1.0501e+00, -8.7840e-01,  1.9587e+00,  1.4944e+00,\n",
      "          7.4874e-02, -4.3279e-01,  3.7451e-01, -1.0749e+00,  1.2737e-01,\n",
      "         -5.4585e-01,  1.6825e+00, -5.8239e-01, -1.8118e+00, -1.3163e-01,\n",
      "          1.1807e-01,  1.5022e+00, -6.4757e-01,  1.2002e+00,  7.3776e-01,\n",
      "          2.4162e+00, -1.3145e+00, -6.3351e-01, -4.6789e-01,  1.2454e+00,\n",
      "          1.3648e-01,  5.3215e-01,  4.4610e-01, -3.2198e-01, -1.0843e+00,\n",
      "          2.5404e-01, -1.2795e+00,  4.8723e-01,  1.4868e+00, -1.6422e+00,\n",
      "         -1.2541e+00, -2.2953e+00,  4.1485e-01, -8.7025e-01,  2.7134e+00,\n",
      "          3.5787e-01,  8.7708e-02, -2.9917e-01,  4.4073e-01, -6.3356e-01,\n",
      "         -9.8800e-03, -8.9385e-02, -1.3608e-01,  3.0560e-01, -9.2311e-01,\n",
      "         -5.4890e-01,  2.0113e+00, -8.1110e-01,  8.6954e-01,  1.1433e-01,\n",
      "         -1.1358e+00, -1.8124e+00, -7.5642e-03, -6.0510e-01,  8.8213e-01,\n",
      "          9.1388e-01, -5.3194e-01, -3.5668e-01, -1.3248e+00, -1.2480e+00,\n",
      "          9.2678e-01,  1.6707e-01,  5.3084e-01, -7.5177e-01,  8.3994e-01,\n",
      "         -7.7172e-01, -9.5351e-01, -3.8438e-01, -1.1972e+00, -6.7503e-02],\n",
      "        [ 7.4859e-01,  6.4490e-01, -2.0788e+00,  2.2606e-01, -1.1603e+00,\n",
      "         -9.7538e-01, -5.4933e-01, -1.1743e+00,  2.6577e-01, -4.7281e-01,\n",
      "          4.3146e-01, -2.4875e+00, -1.4435e+00, -1.2056e-01,  2.1710e+00,\n",
      "          2.2107e-01,  1.3904e+00,  1.6154e+00,  4.3814e-01, -1.3435e+00,\n",
      "          3.2706e-02,  8.4265e-01, -1.2425e+00, -1.0499e+00,  1.8863e+00,\n",
      "         -4.3167e-01, -7.8975e-01,  5.0258e-01, -2.5313e-01, -2.7316e-01,\n",
      "         -7.8571e-01,  5.0244e-01,  1.0144e+00,  1.3915e+00,  1.1649e+00,\n",
      "          1.0338e+00, -3.5423e-01, -5.9066e-02, -4.9270e-01,  4.3470e-01,\n",
      "         -1.2510e+00,  4.0459e-01, -5.2847e-01,  2.8546e-01,  8.2920e-01,\n",
      "         -1.0497e+00, -6.2608e-01,  1.6289e+00,  1.5225e-01,  4.6186e-01,\n",
      "          3.9507e-02,  1.5130e+00, -8.9895e-01, -1.1555e-02,  8.3745e-01,\n",
      "          6.6402e-01, -6.9242e-01, -5.1957e-01,  6.5125e-01, -6.0456e-04,\n",
      "          8.0122e-01, -1.1303e+00, -8.4655e-01,  1.5263e+00, -1.8185e+00,\n",
      "          5.5235e-01,  1.9247e-02,  9.4646e-01, -1.5058e+00, -1.0596e+00,\n",
      "         -8.0018e-01, -3.9529e-01,  8.3077e-02, -4.3088e-01,  3.7177e-01,\n",
      "         -1.3919e+00, -4.6884e-01,  4.1137e-01, -2.0817e+00, -2.2053e-01,\n",
      "         -1.8054e-01, -1.4664e+00,  1.2878e-01, -1.5461e-01,  1.2265e-01,\n",
      "          1.6049e+00,  4.2623e-01, -2.7752e-02, -2.9583e-01,  1.3459e-01,\n",
      "         -5.9506e-01, -5.3402e-01, -8.2125e-02, -1.8458e-01, -6.9052e-01,\n",
      "         -2.3662e-01, -1.0198e+00, -3.9013e-01, -1.1973e+00,  3.3671e-01]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 准备初始权重\n",
    "w1 = torch.randn(input_size, hidden_size, requires_grad=True, device=device, dtype=dtype)\n",
    "w2 = torch.randn(hidden_size, output_size, requires_grad=True, device=device, dtype=dtype)\n",
    "\n",
    "print(w1.grad, w1.grad_fn, w1)\n",
    "# None None tensor([[...]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备偏置项\n",
    "b1 = torch.zeros(1, hidden_size, requires_grad=True, device=device, dtype=dtype)\n",
    "b2 = torch.zeros(1, output_size, requires_grad=True, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_batches = int(len(trX) / batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch支持基于动态图的网络，该网络在每次迭代时计算图\n",
    "# 遍历数据时，实际是在动态地创建图，并在到达最后一个结点或根结点时对其进行反向传播\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(no_of_batches):\n",
    "        start = batch * batches\n",
    "        end = start + batches\n",
    "        x_ = x[start:end]\n",
    "        y_ = y[start:end]\n",
    "\n",
    "        a2 = x_.matmul(w1)\n",
    "        a2 = a2.add(b1)\n",
    "\n",
    "        # print(a2.grad, a2.grad_fn, a2)\n",
    "        # None <AddBackward0 object at 0x7f5f3b9253c8> tensor([[...]])\n",
    "\n",
    "        h2 = a2.sigmoid()\n",
    "\n",
    "        a3 = h2.matmul(w2)\n",
    "        a3 = a3.add(b2)\n",
    "        hyp = a3.sigmoid()\n",
    "\n",
    "        error = hyp - y_\n",
    "        output = error.pow(2).sum() / 2.0\n",
    "        output.backward()\n",
    "\n",
    "        # 向前移动之前，需要检查之前检查过的所有张量\n",
    "        # print(x.grad, x.grad_fn, x)\n",
    "        # None None tensor([[...]])\n",
    "        # print(w1.grad, w1.grad_fn, w1)\n",
    "        # tensor([[...]], None, tensor([[...]]\n",
    "        # print(a2.grad, a2.grad_fn, a2)\n",
    "        # None <AddBackward0 object at 0x7f5f3d42c780> tensor([[...]])\n",
    "\n",
    "        # Direct manipulation of data outside autograd is not allowed\n",
    "        # when grad flag is True\n",
    "        with torch.no_grad():\n",
    "            w1 -= lr * w1.grad\n",
    "            w2 -= lr * w2.grad\n",
    "            b1 -= lr * b1.grad\n",
    "            b2 -= lr * b2.grad\n",
    "        # Making gradients zero. This is essential otherwise, gradient\n",
    "        # from next iteration accumulates\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "        b1.grad.zero_()\n",
    "        b2.grad.zero_()\n",
    "#     if epoch % 10:\n",
    "#         print(epoch, output.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DivBackward0 object at 0x000002A7292D0948>\n",
      "<SumBackward0 object at 0x000002A7292ED988>\n",
      "<PowBackward0 object at 0x000002A7292D0948>\n"
     ]
    }
   ],
   "source": [
    "# traversing the graph using .grad_fn\n",
    "print(output.grad_fn)\n",
    "# <DivBackward0 object at 0x7eff00ae3ef0>\n",
    "print(output.grad_fn.next_functions[0][0])\n",
    "# <SumBackward0 object at 0x7eff017b4128>\n",
    "print(output.grad_fn.next_functions[0][0].next_functions[0][0])\n",
    "# <PowBackward0 object at 0x7eff017b4128>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 21 -- Actual: fiz -- Prediction: number\n",
      "Number: 78 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 866 -- Actual: number -- Prediction: number\n",
      "Number: 710 -- Actual: buz -- Prediction: number\n",
      "Number: 96 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 998 -- Actual: number -- Prediction: number\n",
      "Number: 827 -- Actual: number -- Prediction: number\n",
      "Number: 115 -- Actual: buz -- Prediction: fiz\n",
      "Number: 845 -- Actual: buz -- Prediction: number\n",
      "Number: 137 -- Actual: number -- Prediction: fiz\n",
      "Number: 641 -- Actual: number -- Prediction: number\n",
      "Number: 965 -- Actual: buz -- Prediction: number\n",
      "Number: 994 -- Actual: number -- Prediction: number\n",
      "Number: 485 -- Actual: buz -- Prediction: number\n",
      "Number: 0 -- Actual: fizbuz -- Prediction: fiz\n",
      "Number: 648 -- Actual: fiz -- Prediction: number\n",
      "Number: 740 -- Actual: buz -- Prediction: number\n",
      "Number: 578 -- Actual: number -- Prediction: number\n",
      "Number: 863 -- Actual: number -- Prediction: number\n",
      "Number: 55 -- Actual: buz -- Prediction: number\n",
      "Number: 57 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 847 -- Actual: number -- Prediction: number\n",
      "Number: 820 -- Actual: buz -- Prediction: number\n",
      "Number: 751 -- Actual: number -- Prediction: number\n",
      "Number: 985 -- Actual: buz -- Prediction: number\n",
      "Number: 727 -- Actual: number -- Prediction: number\n",
      "Number: 544 -- Actual: number -- Prediction: number\n",
      "Number: 872 -- Actual: number -- Prediction: number\n",
      "Number: 512 -- Actual: number -- Prediction: number\n",
      "Number: 411 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 425 -- Actual: buz -- Prediction: fiz\n",
      "Number: 231 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 352 -- Actual: number -- Prediction: number\n",
      "Number: 725 -- Actual: buz -- Prediction: number\n",
      "Number: 823 -- Actual: number -- Prediction: fiz\n",
      "Number: 63 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 934 -- Actual: number -- Prediction: number\n",
      "Number: 210 -- Actual: fizbuz -- Prediction: fiz\n",
      "Number: 447 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 437 -- Actual: number -- Prediction: number\n",
      "Number: 3 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 524 -- Actual: number -- Prediction: number\n",
      "Number: 749 -- Actual: number -- Prediction: number\n",
      "Number: 45 -- Actual: fizbuz -- Prediction: fiz\n",
      "Number: 67 -- Actual: number -- Prediction: number\n",
      "Number: 897 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 599 -- Actual: number -- Prediction: number\n",
      "Number: 617 -- Actual: number -- Prediction: number\n",
      "Number: 940 -- Actual: buz -- Prediction: number\n",
      "Number: 381 -- Actual: fiz -- Prediction: number\n",
      "Number: 946 -- Actual: number -- Prediction: number\n",
      "Number: 375 -- Actual: fizbuz -- Prediction: number\n",
      "Number: 249 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 424 -- Actual: number -- Prediction: number\n",
      "Number: 633 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 482 -- Actual: number -- Prediction: number\n",
      "Number: 406 -- Actual: number -- Prediction: number\n",
      "Number: 25 -- Actual: buz -- Prediction: number\n",
      "Number: 278 -- Actual: number -- Prediction: number\n",
      "Number: 483 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 341 -- Actual: number -- Prediction: fiz\n",
      "Number: 586 -- Actual: number -- Prediction: number\n",
      "Number: 907 -- Actual: number -- Prediction: number\n",
      "Number: 736 -- Actual: number -- Prediction: number\n",
      "Number: 505 -- Actual: buz -- Prediction: number\n",
      "Number: 254 -- Actual: number -- Prediction: number\n",
      "Number: 787 -- Actual: number -- Prediction: number\n",
      "Number: 984 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 409 -- Actual: number -- Prediction: fiz\n",
      "Number: 732 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 56 -- Actual: number -- Prediction: number\n",
      "Number: 766 -- Actual: number -- Prediction: number\n",
      "Number: 687 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 660 -- Actual: fizbuz -- Prediction: fiz\n",
      "Number: 407 -- Actual: number -- Prediction: number\n",
      "Number: 469 -- Actual: number -- Prediction: fiz\n",
      "Number: 384 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 299 -- Actual: number -- Prediction: number\n",
      "Number: 594 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 737 -- Actual: number -- Prediction: number\n",
      "Number: 788 -- Actual: number -- Prediction: number\n",
      "Number: 306 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 401 -- Actual: number -- Prediction: buz\n",
      "Number: 281 -- Actual: number -- Prediction: number\n",
      "Number: 986 -- Actual: number -- Prediction: number\n",
      "Number: 472 -- Actual: number -- Prediction: number\n",
      "Number: 639 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 500 -- Actual: buz -- Prediction: number\n",
      "Number: 778 -- Actual: number -- Prediction: number\n",
      "Number: 465 -- Actual: fizbuz -- Prediction: fiz\n",
      "Number: 815 -- Actual: buz -- Prediction: number\n",
      "Number: 459 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 706 -- Actual: number -- Prediction: number\n",
      "Number: 597 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 332 -- Actual: number -- Prediction: number\n",
      "Number: 598 -- Actual: number -- Prediction: number\n",
      "Number: 431 -- Actual: number -- Prediction: number\n",
      "Number: 372 -- Actual: fiz -- Prediction: fiz\n",
      "Number: 826 -- Actual: number -- Prediction: number\n",
      "Number: 391 -- Actual: number -- Prediction: number\n",
      "Test loss:  0.2737432142188828\n",
      "accuracy:  0.69\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "with torch.no_grad():\n",
    "    x = torch.from_numpy(teX).to(device=device, dtype=dtype)\n",
    "    y = torch.from_numpy(teY).to(device=device, dtype=dtype)\n",
    "\n",
    "    a2 = x.matmul(w1)\n",
    "    a2 = a2.add(b1)\n",
    "    h2 = a2.sigmoid()\n",
    "\n",
    "    a3 = h2.matmul(w2)\n",
    "    a3 = a3.add(b2)\n",
    "    hyp = a3.sigmoid()\n",
    "    error = hyp - y\n",
    "    output = error.pow(2).sum() / 2.\n",
    "    outli = ['fizbuz', 'buz', 'fiz', 'number']\n",
    "    for i in range(len(teX)):\n",
    "        num = decoder(teX[i])\n",
    "        print('Number: {} -- Actual: {} -- Prediction: {}'.format(num, check_fizbuz(num), outli[hyp[i].max(0)[1].item()]))\n",
    "    print('Test loss: ', output.item() / len(x))\n",
    "    accuracy = hyp.max(1)[1] == y.max(1)[1]\n",
    "    print('accuracy: ', accuracy.sum().item() / len(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
